{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Querying Reddit\n",
    "\n",
    "\n",
    "My goal for this project is to look into the subreddits lifeprotips and unethicallifeprotips to search for trends in word usage that indicate toxic behavior. I plan to communicate my findings to social media websites, so that they can introduce a model that reads into such behaviors and keys into whether or not said behavior will manifest in real life, or if they are simply troll comments. In essence, this is a look into the insincerity of online posts, and if there are word trends that push a post a certain way. \n",
    "\n",
    "\n",
    "In this notebook I will query reddit posts using the pushshift api. The posts will be formated in json file types that I will key into and turn into lists of dictionaries. From there I will turn them into pandas dataframes and export to a csv.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, csv, json, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the base query syntax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting my initial query url to the pushshift api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the parameters for the query. I will be looking at the subreddits lifeprotips and unethical life protips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {#'searchType':'submission',\n",
    "          'subreddit':'lifeprotips, unethicallifeprotips',\n",
    "          'sort':'desc',\n",
    "          'size':10,\n",
    "#           'before': '10d',\n",
    "#           'after': '168d',\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the url to make sure the query terms are correct and the server is responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status code returned from the server is 200, meaning the query was accepted and there aren't any connection issues. I will check the length of the json file. This is just a test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.json()['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length is 10, as expected. Assessing the file structure for keys of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'telarpita4me',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_2d4kyq4v',\n",
       " 'author_patreon_flair': False,\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1554781838,\n",
       " 'domain': 'self.LifeProTips',\n",
       " 'full_link': 'https://www.reddit.com/r/LifeProTips/comments/bb3auv/lpt_remind_yourself_of_the_good_things_you_have/',\n",
       " 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0},\n",
       " 'id': 'bb3auv',\n",
       " 'is_crosspostable': True,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': True,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 2,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/LifeProTips/comments/bb3auv/lpt_remind_yourself_of_the_good_things_you_have/',\n",
       " 'pinned': False,\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1554781839,\n",
       " 'score': 1,\n",
       " 'selftext': \"Even if it's something as simple as a roof over your head, the bed you sleep in, or the phone you use to browse Reddit with.\\n\\nAlso, acknowledge positive things about yourself. Whether it be your hair, your style of clothing or something you accomplished when you were younger.\\n\\nIt sounds cheesy, but for those of us who focus more on the negative aspects of life, it does help to remind yourself that there are hidden gems in there too and its important to take notice of them.\\n\\nIf you are struggling with something, it's perfectly okay to acknowledge your pain/anger/sadness etc. and allow yourself time to work through it. Reminding yourself of the positives is just to keep you from getting completely bogged down in the negativity.\",\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'LifeProTips',\n",
       " 'subreddit_id': 't5_2s5oq',\n",
       " 'subreddit_subscribers': 15707755,\n",
       " 'subreddit_type': 'public',\n",
       " 'suggested_sort': 'confidence',\n",
       " 'thumbnail': 'self',\n",
       " 'title': 'LPT: Remind yourself of the good things you have.',\n",
       " 'url': 'https://www.reddit.com/r/LifeProTips/comments/bb3auv/lpt_remind_yourself_of_the_good_things_you_have/',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['data'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys of interest are:\n",
    "- author\n",
    "- title\n",
    "- selftext\n",
    "- retrieved_on\n",
    "- subreddit\n",
    "- subreddit_id\n",
    "- created_utc\n",
    "- link_id\n",
    "- parent_id\n",
    "- permalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['author',\n",
    "            'title',\n",
    "            'selftext',\n",
    "            'retrieved_on'\n",
    "            'subreddit',\n",
    "            'subreddit_id',\n",
    "            'created_utc',\n",
    "            'retrieved_on',\n",
    "            'link_id',\n",
    "            'parent_id',\n",
    "            'permalink',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Querying Reddit and saving raw data in .json format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create a logfile and format the file names with a unique timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_format_log(file_path, \n",
    "                        logfile = '../assets/file_log.txt', \n",
    "                        now = round(time.time()), \n",
    "                        file_description = None): \n",
    "   \n",
    "    try:\n",
    "        ext = re.search('(?<!^)(?<!\\.)\\.(?!\\.)', file_path).start() \n",
    "    except:\n",
    "        raise NameError('Please enter a relative path with a file extension.') \n",
    "    \n",
    "    stamp = re.search('(?<!^)(?<!\\.)[a-z]+_[a-z]+(?=\\.)', file_path).start()\n",
    "    formatted_name = f'{file_path[:stamp]}{now}_{file_path[stamp:]}'  \n",
    "    if not file_description:\n",
    "        file_description = f'Pull: {time.asctime(time.gmtime(now))}'\n",
    "    with open(logfile, 'a+') as f:\n",
    "        f.write(f'{formatted_name}: {file_description}\\n')\n",
    "    return formatted_name, now, file_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will collect posts and parse them into a dataframe with the features of interest, saving out the raw data for each pull. After saving each query, which max out at 1000 posts, it runs a time delay and then continues for as many queries as I set.\n",
    "\n",
    "Request loop inspired: [(Source)](https://www.reddit.com/r/pushshift/comments/89pxra/pushshift_api_with_large_amounts_of_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_query(subreddits, n_samples=30000, searchType='submission', before=None, after=None):\n",
    "    url = f'https://api.pushshift.io/reddit/search/submission/'\n",
    "    last_post = round(time.time())\n",
    "    post_list = []\n",
    "    \n",
    "    run = 1\n",
    "    while len(post_list) < n_samples:\n",
    "        \n",
    "        try:\n",
    "            print(f'Starting query {run}')\n",
    "            \n",
    "            params = {#'searchType':searchType,\n",
    "              'subreddit':subreddits,\n",
    "              'sort':'desc',\n",
    "              'size':1000,\n",
    "              'before': last_post-1,\n",
    "              'after':after,\n",
    "             }\n",
    "                \n",
    "            response = requests.get(url, params = params)\n",
    "            posts = response.json()['data']\n",
    "            \n",
    "            if len(posts) == 0:\n",
    "                last_post = last_post\n",
    "            else:\n",
    "                last_post = posts[-1]['created_utc']\n",
    "                post_list.extend(posts)\n",
    "                timestamp = posts[-1]['created_utc']\n",
    "                time.sleep(1) \n",
    "                run += 1\n",
    "        except:\n",
    "            if response.status_code != 200:\n",
    "                return f'Check status. Error code: {response.status_code}'\n",
    "            else:\n",
    "                return 'Error. Pull not completed.'\n",
    "    \n",
    "    formatted_name, now, file_description = filename_format_log(file_path =f'../data/raw_{searchType}s.json', now=timestamp)\n",
    "    with open(formatted_name, 'w+') as f:\n",
    "        json.dump(post_list, f)\n",
    "    \n",
    "    print(f'Saved and completed query and returned {len(post_list)} {searchType}s.')\n",
    "    print(f'Reddit text is ready for processing.')\n",
    "    return print(f'Last timestamp was {timestamp}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the query function to collect 10000 comments from the lifeprotips subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting query 1\n",
      "Starting query 2\n",
      "Starting query 3\n",
      "Starting query 4\n",
      "Starting query 5\n",
      "Starting query 6\n",
      "Starting query 7\n",
      "Starting query 8\n",
      "Starting query 9\n",
      "Starting query 10\n",
      "Saved and completed query and returned 10000 submissions.\n",
      "Reddit text is ready for processing.\n",
      "Last timestamp was 1549858877.\n"
     ]
    }
   ],
   "source": [
    "reddit_query(subreddits='lifeprotips',\n",
    "             n_samples=10000,\n",
    "             searchType='submission',\n",
    "#              before = '10d',\n",
    "#              after = '168d'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1553699454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving my lifeprotips json file as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/1549858877_raw_submissions.json', 'r') as f:\n",
    "    lpt_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lpt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just taking a look at the format of the list of jsons to familiarize myself with the structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'minigunman123',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_63uak',\n",
       " 'author_patreon_flair': False,\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1554088723,\n",
       " 'domain': 'self.LifeProTips',\n",
       " 'full_link': 'https://www.reddit.com/r/LifeProTips/comments/b7wnp4/if_you_load_a_news_website_with_a_paywall_keep/',\n",
       " 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0},\n",
       " 'id': 'b7wnp4',\n",
       " 'is_crosspostable': False,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': False,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 2,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/LifeProTips/comments/b7wnp4/if_you_load_a_news_website_with_a_paywall_keep/',\n",
       " 'pinned': False,\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1554088724,\n",
       " 'score': 1,\n",
       " 'selftext': '[removed]',\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'LifeProTips',\n",
       " 'subreddit_id': 't5_2s5oq',\n",
       " 'subreddit_subscribers': 15663748,\n",
       " 'subreddit_type': 'public',\n",
       " 'suggested_sort': 'confidence',\n",
       " 'thumbnail': 'self',\n",
       " 'title': \"If you load a news website with a paywall, keep refreshing the page and hitting escape quickly so that the paywall doesn't load. As you improve this, you will be able to load the article, but prevent the page from loading the paywall.\",\n",
       " 'url': 'https://www.reddit.com/r/LifeProTips/comments/b7wnp4/if_you_load_a_news_website_with_a_paywall_keep/',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpt_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same for UnethicalLifeProTips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting query 1\n",
      "Starting query 2\n",
      "Starting query 3\n",
      "Starting query 4\n",
      "Starting query 5\n",
      "Starting query 6\n",
      "Starting query 7\n",
      "Starting query 8\n",
      "Starting query 9\n",
      "Starting query 10\n",
      "Saved and completed query and returned 10000 submissions.\n",
      "Reddit text is ready for processing.\n",
      "Last timestamp was 1542760866.\n"
     ]
    }
   ],
   "source": [
    "reddit_query(subreddits='unethicallifeprotips',\n",
    "             n_samples=10000,\n",
    "             searchType='submission',\n",
    "#              before = '10d',\n",
    "#              after = '168d'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/1542760866_raw_submissions.json', 'r') as f:\n",
    "    ulpt_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'ThatNothing',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_3iog3nfl',\n",
       " 'author_patreon_flair': False,\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1554088420,\n",
       " 'domain': 'self.UnethicalLifeProTips',\n",
       " 'full_link': 'https://www.reddit.com/r/UnethicalLifeProTips/comments/b7wm0c/ulpt_request_resume_tips_for_college_grads/',\n",
       " 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0},\n",
       " 'id': 'b7wm0c',\n",
       " 'is_crosspostable': False,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': False,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 0,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'house_only',\n",
       " 'permalink': '/r/UnethicalLifeProTips/comments/b7wm0c/ulpt_request_resume_tips_for_college_grads/',\n",
       " 'pinned': False,\n",
       " 'pwls': 1,\n",
       " 'retrieved_on': 1554088421,\n",
       " 'score': 1,\n",
       " 'selftext': '[removed]',\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'UnethicalLifeProTips',\n",
       " 'subreddit_id': 't5_3cx36',\n",
       " 'subreddit_subscribers': 758816,\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'self',\n",
       " 'title': 'ULPT Request: Resume tips for college grads',\n",
       " 'url': 'https://www.reddit.com/r/UnethicalLifeProTips/comments/b7wm0c/ulpt_request_resume_tips_for_college_grads/',\n",
       " 'whitelist_status': 'house_only',\n",
       " 'wls': 1}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulpt_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the json file into a dataframe containing the features of interest. I have taken out a few features, and will now just be looking at the author, title, subreddit, and the permalink. When I am actually cleaning and modelling, I will likely only be using the title feature and the subreddit feature, which will be changed into a binary 0,1 for LPT and ULPT, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_parse(sample, df):\n",
    "    \n",
    "    col_list = ['author',\n",
    "                'title',\n",
    "                'subreddit',\n",
    "                'permalink',\n",
    "                ]\n",
    "    \n",
    "    df = pd.DataFrame(sample)\n",
    "    df = df[col_list]\n",
    "    \n",
    "    df.rename(columns={'subreddit':'subreddit'}, inplace=True)\n",
    "    \n",
    "    col_order = ['author',\n",
    "                'title',\n",
    "                'subreddit',\n",
    "                'permalink',\n",
    "                ]\n",
    "\n",
    "    return df[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the shape of the dataframe to ensure correct transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lpt = reddit_parse(lpt_list, df = 'df_lpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ulpt = reddit_parse(ulpt_list, df = 'df_ulpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_lpt, df_ulpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the two dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>existentialzebra</td>\n",
       "      <td>As a parent of a baby, smell your baby’s diape...</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>/r/UnethicalLifeProTips/comments/9yym50/as_a_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>shercroft</td>\n",
       "      <td>ULPT Hey kids, rather than search through your...</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>/r/UnethicalLifeProTips/comments/9yyigh/ulpt_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>Jojda6</td>\n",
       "      <td>How to Receive a Double Refund from Amazon</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>/r/UnethicalLifeProTips/comments/9yyc7m/how_to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>TheBaconestCow</td>\n",
       "      <td>ULPT I don't know the specifics, basically whe...</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>/r/UnethicalLifeProTips/comments/9yybak/ulpt_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>TheBaconestCow</td>\n",
       "      <td>I don't know the specifics here, basically whe...</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>/r/UnethicalLifeProTips/comments/9yy7b6/i_dont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                              title  \\\n",
       "19995  existentialzebra  As a parent of a baby, smell your baby’s diape...   \n",
       "19996         shercroft  ULPT Hey kids, rather than search through your...   \n",
       "19997            Jojda6         How to Receive a Double Refund from Amazon   \n",
       "19998    TheBaconestCow  ULPT I don't know the specifics, basically whe...   \n",
       "19999    TheBaconestCow  I don't know the specifics here, basically whe...   \n",
       "\n",
       "                  subreddit                                          permalink  \n",
       "19995  UnethicalLifeProTips  /r/UnethicalLifeProTips/comments/9yym50/as_a_p...  \n",
       "19996  UnethicalLifeProTips  /r/UnethicalLifeProTips/comments/9yyigh/ulpt_h...  \n",
       "19997  UnethicalLifeProTips  /r/UnethicalLifeProTips/comments/9yyc7m/how_to...  \n",
       "19998  UnethicalLifeProTips  /r/UnethicalLifeProTips/comments/9yybak/ulpt_i...  \n",
       "19999  UnethicalLifeProTips  /r/UnethicalLifeProTips/comments/9yy7b6/i_dont...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping my target class, unethicallifeprotips to 1 and lifeprotips to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map({'LifeProTips': 0, 'UnethicalLifeProTips': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape corresponds with expected values. Checking the head of the dataframe to make sure all of the data is correctly labeled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>minigunman123</td>\n",
       "      <td>If you load a news website with a paywall, kee...</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/LifeProTips/comments/b7wnp4/if_you_load_a_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Justinjp126</td>\n",
       "      <td>LPT: So you know those little pieces of detach...</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/LifeProTips/comments/b7wnm4/lpt_so_you_know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheFormulaS</td>\n",
       "      <td>LPT: Use a pistachio shell to help you open a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/LifeProTips/comments/b7wnja/lpt_use_a_pista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NullandRandom</td>\n",
       "      <td>LPT: Whenever you buy food combos but you know...</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/LifeProTips/comments/b7wney/lpt_whenever_yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Serenitybyjan88</td>\n",
       "      <td>LPT: Take a picture or video of yourself doing...</td>\n",
       "      <td>0</td>\n",
       "      <td>/r/LifeProTips/comments/b7wncl/lpt_take_a_pict...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                              title  \\\n",
       "0    minigunman123  If you load a news website with a paywall, kee...   \n",
       "1      Justinjp126  LPT: So you know those little pieces of detach...   \n",
       "2      TheFormulaS  LPT: Use a pistachio shell to help you open a ...   \n",
       "3    NullandRandom  LPT: Whenever you buy food combos but you know...   \n",
       "4  Serenitybyjan88  LPT: Take a picture or video of yourself doing...   \n",
       "\n",
       "   subreddit                                          permalink  \n",
       "0          0  /r/LifeProTips/comments/b7wnp4/if_you_load_a_n...  \n",
       "1          0  /r/LifeProTips/comments/b7wnm4/lpt_so_you_know...  \n",
       "2          0  /r/LifeProTips/comments/b7wnja/lpt_use_a_pista...  \n",
       "3          0  /r/LifeProTips/comments/b7wney/lpt_whenever_yo...  \n",
       "4          0  /r/LifeProTips/comments/b7wncl/lpt_take_a_pict...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into unique titles. I will be removing the reposts, or duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19742"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = 'title', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the valuecounts to make sure my classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9909\n",
       "0    9833\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2896ca0dd4d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.subreddit.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping all posts of their tags, which are ULPT or LPT with a colon following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].map(lambda x: x.lstrip('ULPT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].map(lambda x: x.lstrip('LPT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].map(lambda x: x.lstrip(':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].map(lambda x: x.lstrip(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of unique authors just to make sure that the distribution of words is coming from a variety of authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13917"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['author'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing my combined dataframe to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/combined_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was pretty straightforward. I pulled 10k posts from unethicallifeprotips and 10k posts from lifeprotips. The classes are evenly balanced so my model should be able to perform better than the baseline model of 50%. I removed all tags and following punctuation, and I will explore trends and other data cleaning methods in Notebook 2. My target class is unethicallifeprotips."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
